---
title: "An Introduction to the `logisticPCA` R Package"
author: "Andrew J. Landgraf"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{An Introduction to the `logisticPCA` R Package}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

`logisticPCA` is an R package for dimensionality reduction of binary data. To show how it works, we will create a matrix of rank-1.

```{r setup}
library(logisticPCA)
rows = 100
cols = 10
set.seed(1)
A = rnorm(rows)
B = rnorm(cols)
theta = outer(A, B)
```

Each element in this matrix represents the logit of the probability that that element is a 1, 
$$
\theta_{ij} = \log \frac{p_{ij}}{1 - p_{ij}}.
$$

Next, we generate a binary dataset by using the inverse logit of the $\Theta$ as the probabilities.
```{r create_mat}
mat = (matrix(runif(rows * cols), rows, cols) <= inv.logit.mat(theta)) * 1.0
```

Previously, [Collins et al. (2001)](http://papers.nips.cc/paper/2078-a-generalization-of-principal-components-analysis-to-the-exponential-family.pdf) proposed *exponential family PCA* to extend PCA to binary and other types of data. For binary data, they assume that the logit of the probability matrix can be written as a matrix factorization, 
$$ \mbox{logit}(\mathbf{P}) = \mathbf{A} \mathbf{B}^T, $$
where $\mathbf{A}$ and $\mathbf{B}$ are of a lower rank, $k$. This is the same assumption that we used to generate the binary data. 

We can estimate $\mathbf{A}$ and $\mathbf{B}$ by using `logisticSVD`.
```{r lsvd}
logsvd_mod = logisticSVD(mat, k = 1, main_effects = FALSE)
```

We set `k = 1` because we are assuming that there is a one-dimensional representation (which is the truth). We have set `main_effects = FALSE`. If we instead were to set `main_effects = TRUE`, we would be assuming that there are main effects for the variables, or columns, i.e.
$$ \mbox{logit}(\mathbf{P}) = \mathbf{1}_n \boldsymbol{\mu}^T + \mathbf{A} \mathbf{B}^T. $$

`logsvd_mod` is a S3 object of class `lsvd`. Printing the object gives a summary of the fit.

```{r printlsvd}
logsvd_mod
```

```{r plot}
plot(logsvd_mod, type = "scores")
```



A manuscript describing logistic PCA can be found [here](http://www.stat.osu.edu/~yklee/mss/tr890.pdf).

## Classes
Three types of dimensionality reduction are given. For all the functions, the user must supply the desired dimension `k`. The data must be an `n x d` matrix comprised of binary variables (i.e. all `0`'s and `1`'s).

### Logistic PCA
`logisticPCA()` estimates the natural parameters of a Bernoulli distribution in a lower dimensional space. This is done by projecting the natural parameters from the saturated model. A rank-`k` projection matrix, or equivalently a `d x k` orthogonal matrix `U`, is solved for to minimize the Bernoulli deviance. Since the natural parameters from the saturated model are either negative or positive infinity, an additional tuning parameter `M` is needed to approximate them. You can use `cv.lpca()` to select `M` by cross validation. Typical values are in the range of `3` to `10`. 

`mu` is a main effects vector of length `d` and `U` is the `d x k` loadings matrix.

### Logistic SVD
`logisticSVD()` estimates the natural parameters by a matrix factorization. `mu` is a main effects vector of length `d`, `B` is the `d x k` loadings matrix, and `A` is the `n x k` principal component score matrix.

### Convex Logistic PCA
`convexLogisticPCA()` relaxes the problem of solving for a projection matrix to solving for a matrix in the `k`-dimensional Fantope, which is the convex hull of rank-`k` projection matrices. This has the advantage that the global minumum can be obtained efficiently. The disadvantage is that the `k`-dimensional Fantope solution may have a rank much larger than `k`, which reduces interpretability. It is also necessary to specify `M` in this function.

`mu` is a main effects vector of length `d`, `H` is the `d x d` Fantope matrix, and `U` is the `d x k` loadings matrix, which are the first `k` eigenvectors of `H`.

## Methods
Each of the classes has associated methods to make data analysis easier.

* `print()`: Prints a summary of the fitted model.
* `fitted()`: Fits the low dimensional matrix of either natural parameters or probabilities.
* `predict()`: Predicts the PCs on new data. Can also predict the low dimensional matrix of natural parameters or probabilities on new data.
* `plot()`: Either plots the deviance trace, the first two PC loadings, or the first two PC scores using the package `ggplot2`.

In addition, there are functions for performing cross validation.

* `cv.lpca()`, `cv.lsvd()`, `cv.clpca()`: Run cross validation over the rows of the matrix to assess the fit of `M` and/or `k`.
* `plot.cv()`: Plots the results of the `cv()` method.

